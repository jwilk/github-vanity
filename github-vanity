#!/usr/bin/python3

# Copyright © 2013-2016 Jakub Wilk <jwilk@jwilk.net>
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the “Software”), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import argparse
import concurrent.futures
import errno
import fcntl
import http.client
import json
import logging
import operator
import os
import re
import sys
import urllib.request

user_agent = 'github-vanity (https://github.com/jwilk/github-vanity)'
max_workers = 8

prog = os.path.basename(sys.argv[0])

def create_cache_dir():
    path = os.getenv('XDG_CACHE_HOME')
    if not path.startswith('/'):
        path = os.path.join(os.path.expanduser('~'), '.cache')
    path = os.path.join(path, 'github-vanity')
    os.makedirs(path, 0o700, exist_ok=True)
    return path

def format_user_name(user):
    template = '<{login}>'
    if user.get('name') is not None:
        template = '{name} ' + template
    return template.format(**user)

class Dumper(object):

    def __init__(self):
        self.cache_dir = create_cache_dir()
        self.cache_path = os.path.join(self.cache_dir, 'cache.json')
        self.cache = None
        self.lock_fd = None

    def __enter__(self):
        if self.lock_fd is not None:
            raise RuntimeError('{self!r} is already locked'.format(self=self))
        self.lock_fd = os.open(self.cache_dir, os.O_RDONLY)
        try:
            fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except IOError as exc:
            if exc.errno == errno.EWOULDBLOCK:
                print('{prog}: waiting for the lock...'.format(prog=prog), end='', file=sys.stderr)
                sys.stderr.flush()
                fcntl.flock(self.lock_fd, fcntl.LOCK_EX)
                print('', file=sys.stderr)
            else:
                raise
        try:
            with open(self.cache_path, 'rt', encoding='UTF-8') as fp:
                self.cache = json.load(fp)
        except FileNotFoundError:
            self.cache = {}
        return self

    def __exit__(self, *exc_info):
        if self.lock_fd is None:
            return
        try:
            with open(self.cache_path + '.tmp', 'wt', encoding='UTF-8') as fp:
                json.dump(self.cache, fp)
            os.rename(self.cache_path + '.tmp', self.cache_path)
            self.cache = None
        finally:
            os.close(self.lock_fd)
            os.lock_fd = None

    def _parse_link_header(self, link_header):
        if link_header is None:
            return {}
        items = re.split(r',\s+(?=<)', link_header)
        result = {}
        for item in items:
            match = re.match(r'\A<([^>]+)>; rel="(\w+)"\Z', item)
            (url, rel) = match.groups()
            result[rel] = dict(url=url, rel=rel)
        return result

    def _read_json(self, fp):
        with fp:
            data = fp.read()
            data = data.decode('UTF-8')
            return json.loads(data)

    def _get(self, url, needs=()):
        headers = {'User-Agent': user_agent}
        try:
            cached = self.cache[url]
            for need in needs:
                cached[need]
        except KeyError:
            pass
        else:
            headers['If-None-Match'] = cached['etag']
        request = urllib.request.Request(url, headers=headers)
        try:
            response = urllib.request.urlopen(request)
        except urllib.error.HTTPError as exc:
            if exc.code == 304:
                return cached
            elif exc.code >= 400:
                data = self._read_json(exc.file)
                exc.msg = data['message']
            raise
        data = self._read_json(response)
        links = self._parse_link_header(response.headers['Link'])
        result = dict(
            data=data,
            links=links,
        )
        etag = response.headers['ETag']
        if etag is not None:
            result['etag'] = etag
            self.cache[url] = result
        return result

    def get(self, url):
        return self._get(url)['data']

    def async_get(self, urls):
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            for data in executor.map(self.get, urls):
                yield data

    def get_list(self, url, per_page=100):
        url = '{url}?per_page={n}'.format(url=url, n=per_page)
        data = []
        while True:
            result = self._get(url, needs={'links'})
            data += result['data']
            try:
                url = result['links']['next']['url']
            except KeyError:
                break
        return data

    def get_user(self, login):
        url = 'https://api.github.com/users/{login}'.format(login=login)
        return self.get(url)

    def async_get_users(self, logins):
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            for user in executor.map(self.get_user, logins):
                yield user

    def dump_followers(self, login):
        print('followers:')
        url = 'https://api.github.com/users/{login}/followers'.format(login=login)
        followers = self.get_list(url)
        followers = sorted(followers, key=operator.itemgetter('login'))
        users = self.async_get_users(
            follower['login']
            for follower in followers
        )
        for user in users:
            print('-', format_user_name(user))
        print()

    def dump_repositories(self, login):
        print('repositories:')
        print()
        url = 'https://api.github.com/users/{login}/repos'.format(login=login)
        repos = self.get_list(url)
        repos = sorted(repos, key=operator.itemgetter('name'))
        repos_stargazers = self.async_get(
            repo['stargazers_url'] for repo in repos
        )
        repos_subscribers = self.async_get(
            repo['subscribers_url'] for repo in repos
        )
        for repo, stargazers, subscribers in zip(repos, repos_stargazers, repos_subscribers):
            def userset(users):
                return {
                    u['login']
                    for u in users
                    if u['login'] != login
                }
            followers = userset(stargazers) | userset(subscribers)
            if not followers:
                continue
            print('  {repo}:'.format(repo=repo['name']))
            for follower in sorted(followers):
                follower = self.get_user(follower)
                print('   -', format_user_name(follower))
            print()

    def dump(self, login):
        self.dump_followers(login)
        self.dump_repositories(login)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--debug', action='store_true')
    ap.add_argument('user', metavar='<user>')
    options = ap.parse_args()
    if options.debug:
        http.client.HTTPConnection.debuglevel = 1
    with Dumper() as dumper:
        dumper.dump(options.user)

if __name__ == '__main__':
    main()

# vim:ts=4 sts=4 sw=4 et
